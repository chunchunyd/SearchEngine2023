# crawlers: 数据抓取子系统

建立了一个`XmlSpider`类，用于获取数据并调用`./handlers.py`中的方法对获取到的数据进行解析，解析后的结构化数据存储在在`common`中建立的数据表中。

## 文件结构

```
-文件 views.py
	-类 XmlSpider
	-接口 launch_spider
-文件 handlers.py
	-辅助函数 
        -find_node(一个简单的封装，用于查找节点，如果没有找到，返回一个value为空的default节点;主要是为了解决.get("value")报错)
        -str_to_date(将字符串转换为日期)
        -str_to_int(将字符串转换为整数)
    -功能函数
    	-handle_court
    	-handle_procuratorate
    	......
    	-handle_document
    	-handle_judgment
    	-handle_prosecution
```

## 简单介绍

   接口`launch_spider`被调用后，建立一个`XmlSpider`实例并开启它，`XmlSpider`会从本地指定目录获取法律文书，并判断文书所属类别；
   
   然后，它会根据文书类别调用`handlers.py`中的`handle_document`、`handle_judgment`或`handle_prosecution`，后者会调用其他`handle`函数将文书逐步解析。