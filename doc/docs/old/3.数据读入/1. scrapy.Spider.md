一个典型的继承自scrapy.Spider的爬虫类需要实现以下几个接口:

1. name: 爬虫名称,需要唯一。用于爬虫项目的识别。

```python
name = 'example_spider'
```

2. start_urls: 爬虫开始爬取的url列表。爬虫从这些url开始爬取。

```python
start_urls = ['[http://example.com](http://example.com/)', '[http://example.org](http://example.org/)']
```

3. parse(): 解析start_urls和其他页面中的链接(url)和内容。该方法负责解析网页和提取数据。

```python 
def parse(self, response):
    ...  # 解析网页,提取数据
    ```

4. log: 日志对象。[xn--self-955fz5af1uc15e.log.info](http://xn--self-955fz5af1uc15e.log.info/)()等方法打印日志。

```python
[self.log.info](http://self.log.info/)('Scraping info from %s' % response.url)
```

5. allowed_domains: 允许爬取的域名列表。爬虫只会跟踪这些域名中的链接。

```python  
allowed_domains = ['[example.com](http://example.com/)', '[example.org](http://example.org/)']
``` 

所以,总结来说,一个典型的Spider需要实现:
- name: 爬虫名称  
- start_urls: 初始url列表   
- parse(): 解析网页方法  
- log: 日志对象  
- allowed_domains: 允许域名列表
 
通过这几个接口,一个Spider可以完成基本的爬虫功能:
1) 从start_urls开始爬取  
2) 只跟踪allowed_domains中的链接  
3) 使用parse()方法解析网页和提取数据   
4) 使用log打印日志

除此之外,还可以实现:
- item pipelines: 数据清洗、验证  
- middlewares: 扩展爬虫功能  
- settings: 配置爬虫设置