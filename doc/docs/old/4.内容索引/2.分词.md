## 使用jieba分词对一段字符串str进行分词

```python
import jieba

str = "如果你是一名资讯工作者，或者是在某个领域有所专长，那么你一定会经常浏览网上的问题和回答。"

# 精确模式分词
words = jieba.cut(str, cut_all=False) 
print(" ".join(words))
# 如果 你 是 一名 资讯 工作者 ， 或者 是 在 某个 领域 有所 专长 ， 那么 你 一定 会 经常 浏览 网上 的 问题 和 回答 。

# 全模式分词(更多，但是很多无意义词汇)
words = jieba.cut(str, cut_all=True)  
print(" ".join(words))
# 如果 你 是 一名 资讯 工作 工作者 作者 ， 或者 是 在 某个 领域 有所 专长 ， 那么 你 一定 定会 经常 浏览 网上 的 问题 和 回答 。

# 搜索引擎模式分词
words = jieba.cut_for_search(str)   
print(" ".join(words))
# 如果 你 是 一名 资讯 工作 作者 工作者 ， 或者 是 在 某个 领域 有所 专长 ， 那么 你 一定 会 经常 浏览 网上 的 问题 和 回答 。


# 词性标注 
words = jieba.posseg.cut(str)
for word, flag in words:
    print('%s %s' % (word, flag))
# 我 r    
# 爱 v    
# 北京 ns 
# 天安门 ns  

# 停用词
jieba.analyse.set_stop_words("stop_words.txt")  
# 根据自定义stop_words.txt来过滤停用词
words = jieba.posseg.cut(str) 
for word, flag in words:
    print('%s %s' % (word, flag))
# 北京 ns  
# 天安门 ns
```

`jieba.cut_XXX`返回一个迭代器，`jieba.lcut_XXX`则会返回一个list。

这里，选用`jieba.cut_for_search()`最合适。


## 停用词设置

在`indexed`目录下放了两个文件：`sign.json`和`stop_words.txt`。

前者是总结好的标点符号的集合，以一个list形式存储；后者是手动添加的停用词集合，每一行一个词，没有任何分隔符。

```python
# 获取停用词  
# 先获取符号词  
stop_words = json.load(open(SIGN_WORDS_PATH, 'r', encoding='utf-8'))  
# stop_words.txt文件中，可以不断添加新的停用词  
with open(STOP_WORDS_PATH, 'r', encoding='utf-8') as f:  
	stop_words += f.read().splitlines()
```

## 人名

在测试时发现，在对案例全文进行分词时，jieba对人名的支持很差。

```
# '书记员', '许天', '瑶'
# '原告', '韦斌', '姬'
# '被告', '韦斌', '强'
```

对于这些名字，可以采用手动添加用户词典的方式支持：

```python
jieba.add('许天瑶')
jieba.add('韦斌姬')
jieba.add('韦斌强')
```

考虑到我们已经用模型结构化存储好了所有文书，可以直接从当事人表、代理人表和审判员表里把名字添加到jieba的用户词典中。

```python
# 测试发现, jieba分词对人名支持很差，所以这里从数据库手动添加人名dict  
# 获取所有人名，构建人名dict  
parties = Party.objects.all()  
for party in parties:  
	jieba.add_word(party.name)  
agents = Agent.objects.all()  
for agent in agents:  
	jieba.add_word(agent.name)  
judges = Judge.objects.all()  
for judge in judges:  
	jieba.add_word(judge.name)
```
