遍历每个document，将其全文进行分词后，对结果进行处理：

```python
documents = Document.objects.all()
    cnt = 0
    # 遍历文档
    for document in documents:
        cnt += 1
        print(f'正在处理第{cnt}/{len(documents)}个文档', end='\r')
        # 获取文档内容
        content = document.full_text
        # 用jieba分词
        words = jieba.cut_for_search(content)
        # 去除停用词
        words = [word for word in words if word not in stop_words]
        # 遍历分词结果
        visited_words = set()  # 用于记录已经访问过的词条
        for word in words:
            term = Term.objects.get_or_create(term=word, defaults={'document_count': 0})[0]
            # 如果本文档中第一次访问到该词条，则文档数+1
            if word not in visited_words:
                term.document_count += 1
                visited_words.add(word)
            term.save()
            # 建立倒排索引
            posting = Posting.objects.get_or_create(term=term,
                                                    doc_id=document,
                                                    defaults={'frequency': 0, 'position': 0})[0]
            posting.frequency += 1
            posting.position = content.find(word)
            posting.save()
```